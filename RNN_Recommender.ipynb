{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ktq9fzibdW42"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuEiySH9vc1w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xu2s1c4IdW46"
   },
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3flXhf94dW47",
    "outputId": "44548549-c98b-4ea2-d775-42af8dc437c8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>Country</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Month</th>\n",
       "      <th>total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27270</td>\n",
       "      <td>7</td>\n",
       "      <td>2010-01-12 08:26:00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>PX</td>\n",
       "      <td>85123AY</td>\n",
       "      <td>1</td>\n",
       "      <td>17.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27270</td>\n",
       "      <td>7</td>\n",
       "      <td>2010-01-12 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>PX</td>\n",
       "      <td>71053R</td>\n",
       "      <td>1</td>\n",
       "      <td>23.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27270</td>\n",
       "      <td>9</td>\n",
       "      <td>2010-01-12 08:26:00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>PX</td>\n",
       "      <td>84406BH</td>\n",
       "      <td>1</td>\n",
       "      <td>24.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27270</td>\n",
       "      <td>7</td>\n",
       "      <td>2010-01-12 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>PX</td>\n",
       "      <td>84029GV</td>\n",
       "      <td>1</td>\n",
       "      <td>23.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27270</td>\n",
       "      <td>7</td>\n",
       "      <td>2010-01-12 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>PX</td>\n",
       "      <td>84029EX</td>\n",
       "      <td>1</td>\n",
       "      <td>23.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Quantity         InvoiceDate  UnitPrice Country StockCode  \\\n",
       "0       27270         7 2010-01-12 08:26:00       2.55      PX   85123AY   \n",
       "1       27270         7 2010-01-12 08:26:00       3.39      PX    71053R   \n",
       "2       27270         9 2010-01-12 08:26:00       2.75      PX   84406BH   \n",
       "3       27270         7 2010-01-12 08:26:00       3.39      PX   84029GV   \n",
       "4       27270         7 2010-01-12 08:26:00       3.39      PX   84029EX   \n",
       "\n",
       "   Month  total_price  \n",
       "0      1        17.85  \n",
       "1      1        23.73  \n",
       "2      1        24.75  \n",
       "3      1        23.73  \n",
       "4      1        23.73  "
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "\n",
    "#Basic Cleaning\n",
    "train = train.drop('InvoiceNo', axis = 1)\n",
    "mask = train['Quantity'] <= 0\n",
    "train[mask]['Quatity'] = 0\n",
    "users_train = np.unique(train.CustomerID)\n",
    "\n",
    "train['InvoiceDate'] = pd.to_datetime(train['InvoiceDate'])\n",
    "train['Month'] = train['InvoiceDate'].apply(lambda x : int(x.date().strftime('%m')))\n",
    "train['total_price'] = train['Quantity'] * train['UnitPrice']\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilf9X9DVdW5A"
   },
   "outputs": [],
   "source": [
    "def dictionary(Stocks):\n",
    "    prods = np.unique(Stocks, return_counts = False)\n",
    "\n",
    "    product_dic = {}\n",
    "    for n, prod in enumerate(prods):\n",
    "        product_dic[prod] = n\n",
    "    return product_dic\n",
    "\n",
    "def trip_vector(transaction, prod_dic):\n",
    "    trip_vec = np.zeros(len(prod_dic))\n",
    "    \n",
    "    for product in set(transaction):\n",
    "        trip_vec[prod_dic[product]] = 1\n",
    "    return trip_vec\n",
    "\n",
    "def DataLoader(train_data, test_data, users, filter_data = True, n_trips = 7):\n",
    "    n_users = len(users)\n",
    "    n_prods = len(train_data['trip_vec'].iloc[0])\n",
    "    \n",
    "    #Preparing Output data\n",
    "    y_prods = test_data.groupby(['CustomerID'])['StockCode'].apply(' '.join).reset_index()\n",
    "    \n",
    "    #Preparing Input data\n",
    "    train_data = train_data.sort_values(by = ['CustomerID', 'InvoiceDate'])\n",
    "    users_k = train_data.groupby('CustomerID').count()['StockCode'].reset_index().values\n",
    "    if filter_data:\n",
    "        mask = users_k[:,1] > n_trips\n",
    "        users_k = users_k[mask]\n",
    "    max_length = np.max(users_k[:,1])\n",
    "    x, y = [], []\n",
    "    for user in users_k:\n",
    "        mask = train_data['CustomerID'] == user[0]\n",
    "        mask_t = (y_prods['CustomerID'] == user[0])\n",
    "        if sum(mask_t) != 0:\n",
    "            user_trips = train_data[mask]\n",
    "            y_trips = y_prods[mask_t]\n",
    "            \n",
    "            y_trips = y_trips['StockCode'].apply(lambda x : trip_vector(x.split(), prod_dic).astype(int))\n",
    "            user_trips = user_trips['StockCode'].apply(lambda x : trip_vector(x.split(), prod_dic).astype(int))\n",
    "            user_trips = np.concatenate(user_trips.values).ravel().reshape((user_trips.shape[0], n_prods))\n",
    "\n",
    "            x_user = np.zeros((max_length-user[1], n_prods))\n",
    "            x_user = np.vstack((x_user, user_trips))\n",
    "\n",
    "            x.append(x_user)\n",
    "            y.append(np.concatenate(y_trips.values).ravel())\n",
    "    \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOp1ZtF7dW5C",
    "outputId": "ba9c8781-2a6e-4718-caa9-1f4b0c5cd41e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions in training data:  6852\n",
      "Number of transactions in testing data:  7935\n",
      "x:  (825, 32, 3810) ; y:  (825, 3810)\n"
     ]
    }
   ],
   "source": [
    "prod_dic = dictionary(train.StockCode)\n",
    "\n",
    "trips = train.groupby(['CustomerID', 'InvoiceDate', 'Month'])['StockCode'].apply(' '.join).reset_index()\n",
    "mask = trips['Month'] > 6\n",
    "train_data = trips[~mask]; test_data = trips[mask]\n",
    "\n",
    "print('Number of transactions in training data: ', train_data.shape[0])\n",
    "print('Number of transactions in testing data: ', test_data.shape[0])\n",
    "\n",
    "train_data['trip_vec'] = train_data['StockCode'].apply(lambda x : trip_vector(x.split(), prod_dic).astype(int))\n",
    "train_data.head()\n",
    "\n",
    "x, y = DataLoader(train_data, test_data, users_train, filter_data = False)\n",
    "print('x: ', x.shape, '; y: ', y.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOOOh6xXdW5G"
   },
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3w1KkJydW5H"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYeJDttVdW5J",
    "outputId": "5f6e7673-05ea-4738-97fc-d57fe67d60d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 32, 144)           2278080   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 32, 128)           139776    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 32, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3810)              979170    \n",
      "=================================================================\n",
      "Total params: 4,316,578\n",
      "Trainable params: 4,316,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 144\n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units = lstm_out, dropout_U = 0.2, dropout_W = 0.2,\n",
    "               return_sequences = True, input_shape = (x_train.shape[1], 3810)))\n",
    "model.add(LSTM(units = 128, return_sequences = True))\n",
    "model.add(LSTM(units = 256, return_sequences = True))\n",
    "model.add(LSTM(units = 256))\n",
    "model.add(Dense(x.shape[-1], activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keQ-aau2dW5M",
    "outputId": "da7fce08-7493-4f13-83c0-9ac152fd82da",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 660 samples, validate on 165 samples\n",
      "Epoch 1/10\n",
      "660/660 [==============================] - 30s 45ms/step - loss: 0.4902 - acc: 0.8957 - val_loss: 0.1492 - val_acc: 0.9609\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96091, saving model to ../saved_models/nvacc0.9609_e01.hdf5\n",
      "Epoch 2/10\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 0.1793 - acc: 0.9470 - val_loss: 0.1405 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.96091\n",
      "Epoch 3/10\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.1735 - acc: 0.9470 - val_loss: 0.1411 - val_acc: 0.9607\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.96091\n",
      "Epoch 4/10\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.1724 - acc: 0.9470 - val_loss: 0.1395 - val_acc: 0.9607\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.96091\n",
      "Epoch 5/10\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.1721 - acc: 0.9471 - val_loss: 0.1394 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.96091\n",
      "Epoch 6/10\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.1720 - acc: 0.9471 - val_loss: 0.1398 - val_acc: 0.9607\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.96091\n",
      "Epoch 7/10\n",
      "660/660 [==============================] - 14s 21ms/step - loss: 0.1720 - acc: 0.9470 - val_loss: 0.1399 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.96091\n",
      "Epoch 8/10\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.1722 - acc: 0.9470 - val_loss: 0.1401 - val_acc: 0.9606\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.96091\n",
      "Epoch 9/10\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.1721 - acc: 0.9471 - val_loss: 0.1399 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.96091\n",
      "Epoch 10/10\n",
      "660/660 [==============================] - 14s 21ms/step - loss: 0.1720 - acc: 0.9471 - val_loss: 0.1394 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.96091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da5e1780>"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints = ModelCheckpoint(filepath='../saved_models/nvacc{val_acc:.4f}_e{epoch:02d}.hdf5', \n",
    "                              verbose=1,monitor='val_acc', save_best_only=True)\n",
    "\n",
    "model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 10, batch_size=32,\n",
    "          verbose=1, callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AR-Kh_kldW5Q",
    "outputId": "e19c53cd-239a-4abe-87f2-b43928a5ebc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded!!\n"
     ]
    }
   ],
   "source": [
    "best_model = 'nvacc0.9609_e07.hdf5'\n",
    "threshold = 0.5\n",
    "\n",
    "model = load_model('../saved_models/' + best_model)\n",
    "print('Best model loaded!!')\n",
    "\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HxXioOR-dW5T"
   },
   "source": [
    "## Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07nohDqYdW5U",
    "outputId": "8eb030f7-0dbe-446c-abf6-6638cd0cb856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (445, 32, 3810) ; y:  (445, 3810)\n"
     ]
    }
   ],
   "source": [
    "x, y = DataLoader(train_data, test_data, users_train, filter_data = True, n_trips = 5)\n",
    "print('x: ', x.shape, '; y: ', y.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GzpwS81pdW5X",
    "outputId": "c3d4cf1d-68e8-455c-aa6e-78447fedb426",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 356 samples, validate on 89 samples\n",
      "Epoch 1/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2053 - acc: 0.9310 - val_loss: 0.2026 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.93127, saving model to ../saved_models/nvacc0.9313_e01.hdf5\n",
      "Epoch 2/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2044 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.93127 to 0.93147, saving model to ../saved_models/nvacc0.9315_e02.hdf5\n",
      "Epoch 3/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2038 - acc: 0.9312 - val_loss: 0.2026 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.93147\n",
      "Epoch 4/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2026 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.93147\n",
      "Epoch 5/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2037 - acc: 0.9312 - val_loss: 0.2026 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.93147 to 0.93155, saving model to ../saved_models/nvacc0.9315_e05.hdf5\n",
      "Epoch 6/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2025 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.93155\n",
      "Epoch 7/100\n",
      "356/356 [==============================] - 13s 36ms/step - loss: 0.2036 - acc: 0.9312 - val_loss: 0.2026 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.93155\n",
      "Epoch 8/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.93155\n",
      "Epoch 9/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2025 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.93155\n",
      "Epoch 10/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2024 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.93155\n",
      "Epoch 11/100\n",
      "356/356 [==============================] - 12s 35ms/step - loss: 0.2037 - acc: 0.9312 - val_loss: 0.2027 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.93155\n",
      "Epoch 12/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.93155\n",
      "Epoch 13/100\n",
      "356/356 [==============================] - 12s 35ms/step - loss: 0.2038 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.93155\n",
      "Epoch 14/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.93155\n",
      "Epoch 15/100\n",
      "356/356 [==============================] - 12s 34ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2025 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.93155\n",
      "Epoch 16/100\n",
      "356/356 [==============================] - 13s 35ms/step - loss: 0.2038 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.93155\n",
      "Epoch 17/100\n",
      "356/356 [==============================] - 13s 38ms/step - loss: 0.2034 - acc: 0.9313 - val_loss: 0.2026 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.93155\n",
      "Epoch 18/100\n",
      "356/356 [==============================] - 13s 37ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.93155\n",
      "Epoch 19/100\n",
      "356/356 [==============================] - 13s 36ms/step - loss: 0.2038 - acc: 0.9312 - val_loss: 0.2032 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.93155\n",
      "Epoch 20/100\n",
      "356/356 [==============================] - 13s 36ms/step - loss: 0.2038 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.93155\n",
      "Epoch 21/100\n",
      "356/356 [==============================] - 13s 37ms/step - loss: 0.2038 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.93155\n",
      "Epoch 22/100\n",
      "356/356 [==============================] - 12s 33ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2026 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.93155\n",
      "Epoch 23/100\n",
      "356/356 [==============================] - 13s 37ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.93155\n",
      "Epoch 24/100\n",
      "356/356 [==============================] - 12s 34ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.93155\n",
      "Epoch 25/100\n",
      "356/356 [==============================] - 12s 33ms/step - loss: 0.2033 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.93155\n",
      "Epoch 26/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.93155\n",
      "Epoch 27/100\n",
      "356/356 [==============================] - 12s 35ms/step - loss: 0.2037 - acc: 0.9312 - val_loss: 0.2030 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.93155\n",
      "Epoch 28/100\n",
      "356/356 [==============================] - 12s 33ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.93155\n",
      "Epoch 29/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2026 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.93155 to 0.93156, saving model to ../saved_models/nvacc0.9316_e29.hdf5\n",
      "Epoch 30/100\n",
      "356/356 [==============================] - 12s 34ms/step - loss: 0.2036 - acc: 0.9312 - val_loss: 0.2030 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.93156\n",
      "Epoch 31/100\n",
      "356/356 [==============================] - 13s 36ms/step - loss: 0.2038 - acc: 0.9313 - val_loss: 0.2032 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.93156\n",
      "Epoch 32/100\n",
      "356/356 [==============================] - 13s 36ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.93156\n",
      "Epoch 33/100\n",
      "356/356 [==============================] - 12s 33ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.93156\n",
      "Epoch 34/100\n",
      "356/356 [==============================] - 12s 34ms/step - loss: 0.2038 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.93156\n",
      "Epoch 35/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2042 - acc: 0.9312 - val_loss: 0.2032 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.93156\n",
      "Epoch 36/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2039 - acc: 0.9313 - val_loss: 0.2030 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.93156\n",
      "Epoch 37/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2038 - acc: 0.9312 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.93156\n",
      "Epoch 38/100\n",
      "356/356 [==============================] - 12s 34ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.93156\n",
      "Epoch 39/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2035 - acc: 0.9314 - val_loss: 0.2028 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.93156\n",
      "Epoch 40/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.93156\n",
      "Epoch 41/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2030 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.93156\n",
      "Epoch 42/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2026 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.93156\n",
      "Epoch 43/100\n",
      "356/356 [==============================] - 11s 30ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.93156\n",
      "Epoch 44/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.93156\n",
      "Epoch 45/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.93156\n",
      "Epoch 46/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.93156\n",
      "Epoch 47/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2034 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.93156\n",
      "Epoch 48/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.93156\n",
      "Epoch 49/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2026 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.93156\n",
      "Epoch 50/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2037 - acc: 0.9312 - val_loss: 0.2030 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.93156\n",
      "Epoch 51/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.93156\n",
      "Epoch 52/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2034 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.93156\n",
      "Epoch 53/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2026 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.93156\n",
      "Epoch 54/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2037 - acc: 0.9312 - val_loss: 0.2031 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.93156\n",
      "Epoch 55/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2034 - acc: 0.9314 - val_loss: 0.2028 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.93156\n",
      "Epoch 56/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.93156\n",
      "Epoch 57/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2039 - acc: 0.9311 - val_loss: 0.2028 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.93156\n",
      "Epoch 58/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2030 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.93156\n",
      "Epoch 59/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2038 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.93156 to 0.93158, saving model to ../saved_models/nvacc0.9316_e59.hdf5\n",
      "Epoch 60/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2039 - acc: 0.9313 - val_loss: 0.2030 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.93158\n",
      "Epoch 61/100\n",
      "356/356 [==============================] - 11s 30ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.93158\n",
      "Epoch 62/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2035 - acc: 0.9314 - val_loss: 0.2028 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.93158\n",
      "Epoch 63/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2034 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.93158\n",
      "Epoch 64/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2027 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.93158\n",
      "Epoch 65/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.93158\n",
      "Epoch 66/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2038 - acc: 0.9313 - val_loss: 0.2034 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.93158\n",
      "Epoch 67/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.93158\n",
      "Epoch 68/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2037 - acc: 0.9312 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.93158\n",
      "Epoch 69/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2030 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.93158\n",
      "Epoch 70/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2030 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.93158\n",
      "Epoch 71/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2030 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.93158\n",
      "Epoch 72/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2036 - acc: 0.9312 - val_loss: 0.2028 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.93158\n",
      "Epoch 73/100\n",
      "356/356 [==============================] - 13s 35ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.93158\n",
      "Epoch 74/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.93158\n",
      "Epoch 75/100\n",
      "356/356 [==============================] - 14s 39ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.93158\n",
      "Epoch 76/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2040 - acc: 0.9312 - val_loss: 0.2033 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.93158\n",
      "Epoch 77/100\n",
      "356/356 [==============================] - 13s 37ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.93158\n",
      "Epoch 78/100\n",
      "356/356 [==============================] - 12s 35ms/step - loss: 0.2034 - acc: 0.9313 - val_loss: 0.2032 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.93158\n",
      "Epoch 79/100\n",
      "356/356 [==============================] - 14s 38ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.93158\n",
      "Epoch 80/100\n",
      "356/356 [==============================] - 14s 40ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.93158\n",
      "Epoch 81/100\n",
      "356/356 [==============================] - 14s 39ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2030 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.93158\n",
      "Epoch 82/100\n",
      "356/356 [==============================] - 14s 40ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.93158\n",
      "Epoch 83/100\n",
      "356/356 [==============================] - 15s 43ms/step - loss: 0.2036 - acc: 0.9312 - val_loss: 0.2027 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.93158\n",
      "Epoch 84/100\n",
      "356/356 [==============================] - 15s 43ms/step - loss: 0.2035 - acc: 0.9314 - val_loss: 0.2030 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.93158\n",
      "Epoch 85/100\n",
      "356/356 [==============================] - 15s 43ms/step - loss: 0.2038 - acc: 0.9313 - val_loss: 0.2032 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00085: val_acc improved from 0.93158 to 0.93159, saving model to ../saved_models/nvacc0.9316_e85.hdf5\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356/356 [==============================] - 15s 43ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.93159\n",
      "Epoch 87/100\n",
      "356/356 [==============================] - 15s 44ms/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.93159\n",
      "Epoch 88/100\n",
      "356/356 [==============================] - 17s 47ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2031 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.93159\n",
      "Epoch 89/100\n",
      "356/356 [==============================] - 4705s 13s/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.93159\n",
      "Epoch 90/100\n",
      "356/356 [==============================] - 25s 71ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.93159\n",
      "Epoch 91/100\n",
      "356/356 [==============================] - 179s 502ms/step - loss: 0.2033 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.93159\n",
      "Epoch 92/100\n",
      "356/356 [==============================] - 21s 59ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.93159\n",
      "Epoch 93/100\n",
      "356/356 [==============================] - 15s 42ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.93159\n",
      "Epoch 94/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2038 - acc: 0.9314 - val_loss: 0.2030 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.93159\n",
      "Epoch 95/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2034 - acc: 0.9313 - val_loss: 0.2029 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.93159\n",
      "Epoch 96/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2032 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.93159\n",
      "Epoch 97/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2035 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.93159\n",
      "Epoch 98/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2034 - acc: 0.9313 - val_loss: 0.2028 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.93159\n",
      "Epoch 99/100\n",
      "356/356 [==============================] - 11s 32ms/step - loss: 0.2037 - acc: 0.9314 - val_loss: 0.2030 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.93159\n",
      "Epoch 100/100\n",
      "356/356 [==============================] - 11s 31ms/step - loss: 0.2036 - acc: 0.9313 - val_loss: 0.2031 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.93159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da5ef780>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints = ModelCheckpoint(filepath='../saved_models/nvacc{val_acc:.4f}_e{epoch:02d}.hdf5',\n",
    "                                  verbose=1,monitor='val_acc', save_best_only=True)\n",
    "\n",
    "model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 100, batch_size=32,\n",
    "          verbose=1, callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4FQBcWadW5b"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "users_test = np.unique(test.CustomerID)\n",
    "\n",
    "data = pd.concat([train, test], sort=False)\n",
    "del train, test\n",
    "\n",
    "prods = np.unique(data.StockCode, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZ5yeFGZdW5g",
    "outputId": "468732a1-8f8e-4c20-dad1-6efd3a94685f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>StockCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22857</th>\n",
       "      <td>577215</td>\n",
       "      <td>02/12/11 2:02:00 PM</td>\n",
       "      <td>46000MP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22858</th>\n",
       "      <td>577215</td>\n",
       "      <td>08/12/11 1:36:00 PM</td>\n",
       "      <td>22429T, 84378P, 23250A, 22720A, 23366F, 22972W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22859</th>\n",
       "      <td>578025</td>\n",
       "      <td>27/11/11 4:18:00 PM</td>\n",
       "      <td>22483A, 23583L, 21793O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22860</th>\n",
       "      <td>579015</td>\n",
       "      <td>01/12/11 3:11:00 PM</td>\n",
       "      <td>23499W, 22831H, 22079R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22861</th>\n",
       "      <td>580815</td>\n",
       "      <td>07/12/11 8:03:00 AM</td>\n",
       "      <td>22961C, 21531U, 22627E, 22625P, 22960K, 22554P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CustomerID          InvoiceDate  \\\n",
       "22857      577215  02/12/11 2:02:00 PM   \n",
       "22858      577215  08/12/11 1:36:00 PM   \n",
       "22859      578025  27/11/11 4:18:00 PM   \n",
       "22860      579015  01/12/11 3:11:00 PM   \n",
       "22861      580815  07/12/11 8:03:00 AM   \n",
       "\n",
       "                                               StockCode  \n",
       "22857                                            46000MP  \n",
       "22858  22429T, 84378P, 23250A, 22720A, 23366F, 22972W...  \n",
       "22859                             22483A, 23583L, 21793O  \n",
       "22860                             23499W, 22831H, 22079R  \n",
       "22861  22961C, 21531U, 22627E, 22625P, 22960K, 22554P...  "
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips = data.groupby(['CustomerID', 'InvoiceDate'])['StockCode'].apply(', '.join).reset_index()\n",
    "trips.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iUEJv2WYdW5j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RNN_Recommender.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
